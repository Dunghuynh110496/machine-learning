{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition:\n",
    "$$\\eqalign{\n",
    "  & \\bar y = \\arg \\mathop {\\max }\\limits_y p\\left( {Y = y|X = x} \\right) = \\arg \\mathop {\\max }\\limits_y \\frac{{p\\left( {Y = y} \\right)p(X = x|Y = y)}}{{P(X = x)}}  \\cr \n",
    "  &  = \\arg \\mathop {\\max }\\limits_y p\\left( {Y = y} \\right)\\prod\\limits_{i = 1}^n {p({x_i}|Y = y) = \\arg \\mathop {\\max }\\limits_y \\ln \\left( {p\\left( {Y = y} \\right)p({x_i}|Y = y)} \\right)}   \\cr \n",
    "  &  = \\arg \\mathop {\\max }\\limits_y \\ln p\\left( {Y = y} \\right) + \\sum\\limits_{i = 1}^d {\\ln } p({x_i}|Y = y) \\cr} $$\n",
    "\n",
    "${\\bar y}$ is the prediction\n",
    "\n",
    "The first equal symbol: $y_i$ is the label that has the highest probability given vector x.\n",
    "\n",
    "The second equal symbol: it is the result of Bayes Theorem.\n",
    "\n",
    "The third equal symbol: it is due to $x_i$ being independent with each other. However, in practice, it is impossible. Therefore, it is called naive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to fit it:\n",
    "\n",
    "We calculate the the prior distribution, variances and the mean of all features of each label.\n",
    "$${\\mu _{y,i}} = \\frac{{sum\\,of\\,values\\,of\\,{x_i}\\,in\\,obvervation\\,y}}{{\\# obvervation\\,of\\,y}}$$\n",
    "\\\n",
    "\\\n",
    "$${\\sigma ^2}_{y,i} = \\frac{{\\sum\\limits_{j = 1}^{\\# obvervation\\,\\,\\,of\\,\\,y} {({x_{j,i}} - {\\mu _{y,i}})} }}{{\\# obvervation\\,\\,of\\,y\\,\\,\\,\\,}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use it:\n",
    "1. Separate the samples of the labels into different matrixs.\n",
    "2. Calculate the prior distribution, mean and variances of each label and put them into a dictionary.\n",
    "3. Find the class-conditional distribution of the labels by using the below formula with $x_i$ is the element from input:\n",
    "$$ p\\left( {{x_i}|y} \\right) = \\frac{1}{{\\sqrt {2\\pi } {\\sigma _{y,i}}}}{e^{ - \\frac{{{{\\left( {{x_i} - {\\mu _{y,i}}} \\right)}^2}}}{{2{\\sigma _{y,i}}^2}}}}$$\n",
    "4. Calculate the probability of each label given the input.\n",
    "5. Return the label that has the highest probability from 4th step as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from math import pi\n",
    "import numpy as np\n",
    "class NaiveBayer:\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef fit(self,X,y):\n",
    "\t\tself.X = X\n",
    "\t\tself.y = y\n",
    "\t\tself.labels = np.unique(y)\n",
    "\t\tself.summaries = {}\n",
    "\t\tfor label in self.labels:\n",
    "\t\t\tX_copy = X[y == label]\n",
    "\t\t\tmeans = X_copy.mean(axis = 0)\n",
    "\t\t\tstd = X_copy.std(axis = 0)\n",
    "\t\t\tprior = len(X_copy)/len(X)\n",
    "\t\t\tself.summaries[label] = (prior, means , std)\n",
    "\n",
    "\tdef predict(self, x):\n",
    "\t\tpredictions = {}\n",
    "\t\tfor label in self.labels:\n",
    "\t\t\tprior_probability, mean, std = np.log(self.summaries[label][0]), self.summaries[label][1], self.summaries[label][2]\n",
    "\t\t\tfor idx in range(len(x)):\n",
    "\t\t\t\tif mean[idx] != 0:\n",
    "\t\t\t\t\tprior_probability += -((x[idx] - mean[idx])**2 / (2 * std[idx]**2 )) - np.log(std[idx])\n",
    "\t\t\tpredictions[label] = prior_probability\n",
    "\n",
    "\t\ta = float('-inf')\n",
    "\t\tfor key in predictions:\n",
    "\t\t\tif predictions[key] > a:\n",
    "\t\t\t\ta = predictions[key]\n",
    "\t\t\t\tprediction = key\n",
    "\t\treturn prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train = pd.read_csv(\"../data/digit-recognizer/data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/digit-recognizer/data/test.csv\")\n",
    "\n",
    "train = df_train.to_numpy()\n",
    "X = train[:,1:]\n",
    "y = train[:,0]\n",
    "classifier = NaiveBayer()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "f = open(\"../data/digit-recognizer/outputs/submission_nb.csv\", \"w\")\n",
    "f.write(\"ImageId,Label\\n\" )\n",
    "test = df_test.to_numpy()\n",
    "for i in range(len(test)):\n",
    "    a = classifier.predict(test[i])\n",
    "    f.write(str(i+1) + \",\" + str(a) +\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and tranform the data to fit effectively\n",
    "df_train = pd.read_csv(\"../data/digit-recognizer/data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/digit-recognizer/data/test.csv\")\n",
    "train = df_train.to_numpy()\n",
    "test = df_test.to_numpy()\n",
    "scaler = MinMaxScaler()\n",
    "X = train[:,1:]\n",
    "y = train[:,0]\n",
    "scaler.fit(X)\n",
    "X_transform = scaler.transform(X)\n",
    "test_transform = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8248809523809524\n",
      "0.7165\n",
      "0.7165\n",
      "0.8347857142857142\n"
     ]
    }
   ],
   "source": [
    "NaiveBayer1 = GaussianNB()\n",
    "NaiveBayer1.fit(X_transform, y)\n",
    "NaiveBayer1.score(X_transform, y)\n",
    "print(NaiveBayer2.score(X_transform, y))\n",
    "\n",
    "NaiveBayer2 = MultinomialNB()\n",
    "NaiveBayer2.fit(X_transform, y)\n",
    "print(NaiveBayer3.score(X_transform, y))\n",
    "\n",
    "NaiveBayer3 = ComplementNB()\n",
    "NaiveBayer3.fit(X_transform, y)\n",
    "print(NaiveBayer3.score(X_transform, y))\n",
    "\n",
    "NaiveBayer4 = BernoulliNB()\n",
    "NaiveBayer4.fit(X_transform, y)\n",
    "print(NaiveBayer4.score(X_transform, y))\n",
    "\n",
    "\n",
    "predictions = NaiveBayer4.predict(test_transform)\n",
    "\n",
    "f = open(\"../data/digit-recognizer/outputs/submission_NaiveBayer_sklearn.csv\", \"w\")\n",
    "f.write(\"ImageId,Label\\n\" )\n",
    "for i in range(len(predictions)):\n",
    "    f.write(str(i+1) + \",\" + str(predictions[i]) +\"\\n\")\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
