{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition:\n",
    "\n",
    "As with regression, absorb the offset:\n",
    "$w \\leftarrow \\left[ \\begin{array}{l}\n",
    "{w_0}\\\\\n",
    "w\n",
    "\\end{array} \\right]$ and \n",
    "$x \\leftarrow \\left[ \\begin{array}{l}\n",
    "1\\\\\n",
    "x\n",
    "\\end{array} \\right]$\n",
    "\n",
    "\n",
    "Let $ \\left( {{x_1},{y_1}} \\right),...,\\left( {{x_n},{y_n}} \\right) $  be a set of binary labeled data with $ y \\in \\left\\{ { - 1, + 1} \\right\\} $. Logistic Regression models each $y_i$ as independently generated, with:\n",
    "\n",
    "$$P\\left( {{y_i} =  + 1|{x_i},w} \\right) = \\sigma \\left( {{x_i}^Tw} \\right),\\,\\,\\,\\,\\,\\sigma \\left( {{x_i}^Tw} \\right)\\, = \\frac{{{e^{{x_i}^Tw}}}}{{1 + {e^{{x_i}^Tw}}}}$$\n",
    "\n",
    "Predicting new data is the same:\n",
    "- If ${x^T}w > 0$, then $\\sigma ({x^T}w) > 1/ 2$ and predict $y =  + 1$ and vice versa.\n",
    "\n",
    "- We now get a confidence in our prediction via the probability $\\sigma ({x^T}w)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to fit it:\n",
    "Firstly, we find $w$ that maximize the likelihood ($p\\left( {{y_1},...,{y_n}|{x_1},...,{x_n},w} \\right)$) to caculate the prediction. \n",
    "\n",
    "\n",
    "\n",
    "The joint likelihood of ${y_1},...,{y_n}$ is:\n",
    "\n",
    "$$p\\left( {{y_1},...,{y_n}|{x_1},...,{x_n},w} \\right) = \\prod\\limits_{i = 1}^n {p\\left( {{y_i}|{x_i},w} \\right)}$$\n",
    "\n",
    "Notice that each $x_i$ modifies the probability of success for its $y_i$.\n",
    "\n",
    "We have :\n",
    "\n",
    "$\\sigma \\left( {{y_i}w} \\right) = \\sigma \\left( {{y_i}{x_i}^Tw} \\right)$\n",
    "\n",
    "\n",
    "= $\\frac{{{e^{{y_i}{x_i}^Tw}}}}{{1 + {e^{{y_i}{x_i}^Tw}}}}$ = ${\\left( {\\frac{{{e^{{x_i}^T}}w}}{{1 + {e^{{x_i}^Tw}}}}} \\right)^{L({y_i} =  + 1)}}$ + ${\\left( {\\frac{{{e^{{x_i}^T}}w}}{{1 + {e^{{x_i}^Tw}}}}} \\right)^{L\\left( {{y_i} =  - 1} \\right)}}$\n",
    "\n",
    "$ = {\\sigma _i}{\\left( w \\right)^{L\\left( {{y_i} =  + 1} \\right)}}{\\left( {1 - {\\sigma _i}\\left( w \\right)} \\right)^{L\\left( {{y_i} =  - 1} \\right)}}$\n",
    "\n",
    "\n",
    "\n",
    "Therefore:\n",
    "$p\\left( {{y_1},...,{y_n}|{x_1},...,{x_n},w} \\right) = \\prod\\limits_{i = 1}^n {{\\sigma _i}{{\\left( w \\right)}^{L\\left( {{y_i} =  + 1} \\right)}}} {\\left( {1 - {\\sigma _i}\\left( w \\right)} \\right)^{L\\left( {{y_i} =  - 1} \\right)}} = \\prod\\limits_{i = 1}^n {\\sigma \\left( {{y_i}.w} \\right)} $\n",
    "\n",
    "Now, we will maximize $\\prod\\limits_{i = 1}^n {\\sigma \\left( {{y_i}.w} \\right)} $ over $w$.\n",
    "\n",
    "We have:\n",
    "\n",
    "${w_{ML}} = \\arg \\mathop {\\max }\\limits_w \\prod\\limits_{i = 1}^n {\\sigma \\left( {{y_i}.w} \\right)}  = \\arg \\mathop {\\max }\\limits_w \\ln \\left( {\\prod\\limits_{i = 1}^n {\\sigma \\left( {{y_i}.w} \\right)} } \\right) = \\arg \\mathop {\\max }\\limits_w \\sum\\limits_{i = 1}^n {\\ln {\\sigma _i}\\left( {{y_i}.w} \\right)}  = \\arg \\mathop {\\max }\\limits_w L$\n",
    "\n",
    "With ${L} = \\sum\\limits_{i = 1}^n {\\ln {\\sigma _i}\\left( {{y_i}.w} \\right)} $.\n",
    "\n",
    "To find ${w_{ML}} = \\arg \\mathop {\\max }\\limits_w L$, we take the gradient of $L$\n",
    "\n",
    "$\\eqalign{\n",
    "  & {\\nabla _w}L = {\\nabla _w}\\sum\\limits_{i = 1}^n {\\ln \\left( {\\frac{{{e^{{y_i}{x_i}^Tw}}}}{{1 + {e^{^{{y_i}{x_i}^Tw}}}}}} \\right)}  = {\\nabla _w}\\sum\\limits_{i = 1}^n {\\ln \\left( {{e^{{y_i}{x_i}^Tw}}} \\right) - \\ln \\left( {1 + {e^{^{{y_i}{x_i}^Tw}}}} \\right)}  = {\\nabla _w}\\sum\\limits_{i = 1}^n {{y_i}{x_i}^Tw - \\ln \\left( {1 + {e^{^{{y_i}{x_i}^Tw}}}} \\right)}   \\cr \n",
    "  &  = {\\nabla _w}\\sum\\limits_{i = 1}^n {{y_i}{x_i}^Tw - \\ln \\left( {1 + {e^{^{{y_i}{x_i}^Tw}}}} \\right)}  = \\sum\\limits_{i = 1}^n {{\\nabla _w}\\left( {{y_i}{x_i}^Tw} \\right)}  - {\\nabla _w}\\ln \\left( {1 + {e^{^{{y_i}{x_i}^Tw}}}} \\right) = \\sum\\limits_{i = 1}^n {{y_i}{x_i}^T}  - \\frac{{{\\nabla _w}\\left( {{e^{^{{y_i}{x_i}^Tw}}}} \\right)}}{{1 + {e^{^{{y_i}{x_i}^Tw}}}}}  \\cr \n",
    "  &  = \\sum\\limits_{i = 1}^n {{y_i}{x_i}^T}  - \\frac{{{y_i}{x_i}^T.{e^{^{{y_i}{x_i}^Tw}}}}}{{1 + {e^{^{{y_i}{x_i}^Tw}}}}} = \\sum\\limits_{i = 1}^n {{y_i}{x_i}^T} \\left( {1 - \\frac{{{e^{^{{y_i}{x_i}^Tw}}}}}{{1 + {e^{^{{y_i}{x_i}^Tw}}}}}} \\right) = \\sum\\limits_{i = 1}^n {{y_i}{x_i}^T} \\left( {1 - \\sigma \\left( {{y_i}.w} \\right)} \\right) = \\sum\\limits_{i = 1}^n {\\left( {1 - \\sigma \\left( {{y_i}.w} \\right)} \\right)} {y_i}{x_i} \\cr} $\n",
    "\n",
    "As with the Perceptron, we canâ€™t directly set ${\\nabla _w}L = 0$, so we need an\n",
    "iterative algorithm. At step $t$, we can update\n",
    "\n",
    "$${w^{\\left( {t + 1} \\right)}} = {w^{\\left( t \\right)}} + \\eta {\\nabla _w}L,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,{\\nabla _w}L = \\sum\\limits_{i = 1}^n {\\left( {1 - {\\sigma _i}\\left( {{y_i}.w} \\right)} \\right){y_i}{x_i}.} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use it\n",
    "\n",
    "Input: training data $\\left( {{x_1},{y_1}} \\right),...,\\left( {{x_n},{y_n}} \\right)$ and step size $\\eta  > 0$:\n",
    "1. Set ${w^{(1)}} = \\vec 0$.\n",
    "2. For step $t = 1,2,...$ do:\n",
    "  - Update ${w^{\\left( {t + 1} \\right)}} = {w^{\\left( t \\right)}} + \\eta \\sum\\limits_{i = 1}^n {\\left( {1 - {\\sigma _i}\\left( {{y_i}.w} \\right)} \\right){y_i}{x_i}} $\n",
    "3. Create a dictionary to save all weights of the labels.\n",
    "4. Calculate the sigmoid probabilities of the input and each weight of each label.\n",
    "5. Return the label that has the highest probability as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as exp\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "class LogisticRegression:\n",
    "\tdef __init__(self,n, condition,max_epoch ,batch_size):\n",
    "\t\tself.n = n\n",
    "\t\tself.condition =  condition\n",
    "\t\tself.max_epoch = max_epoch \n",
    "\t\tself.batch_size = batch_size\n",
    "\n",
    "\tdef fit(self, X, y):\n",
    "\t\tself.X = X\n",
    "\t\tX = np.insert(X,0,1,axis=1)\n",
    "\t\tself.y = y\n",
    "\t\tself.labels = np.unique(y)\n",
    "\t\tself.labels.sort()\n",
    "\t\tself.w_classes = {label : np.random.normal(0, 1, len(X[0]))for label in self.labels}\n",
    "\t\tself.check_list = []\n",
    "\t\tnum_batch = len(X)//self.batch_size\n",
    "\t\tfor k in range(self.max_epoch):\n",
    "\t\t\tgradient_classes = {label: np.zeros(len(X[0])) for label in self.labels}\n",
    "\t\t\tidx = list(range(len(X)))\n",
    "\t\t\tnp.random.shuffle(idx)\n",
    "\t\t\tX = X[idx]\n",
    "\t\t\ty = y[idx] \n",
    "\t\t\tfor i in range(num_batch):\n",
    "\t\t\t\tfor label in self.labels:\n",
    "\t\t\t\t\tif label not in self.check_list:\n",
    "\t\t\t\t\t\tsign = [1 if j == label else -1 for j in y[self.batch_size*i:self.batch_size*(i+1)]]\n",
    "\t\t\t\t\t\tsigmoid = self.sigmoid( np.matmul(X[self.batch_size*i:self.batch_size*(i+1)],self.w_classes[label])*sign)\n",
    "\t\t\t\t\t\tgradient_matrix = ((1 - sigmoid)*sign).reshape((self.batch_size,1))*X[self.batch_size*i:self.batch_size*(i+1)]\n",
    "\t\t\t\t\t\tgradient_classes[label] =  np.sum((gradient_matrix), axis=0)\n",
    "\t\t\t\t\t\tif LA.norm(gradient_classes[label]) > self.condition:\n",
    "\t\t\t\t\t\t\tself.w_classes[label] += self.n*gradient_classes[label]\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tself.check_list.append(label)\n",
    "\t\t\t\tif len(self.check_list) == len(self.labels):\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tif len(self.check_list) == len(self.labels):\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\tdef sigmoid(self, p):\n",
    "\t\tprobability = 1 / (1 + np.exp(- p))\n",
    "\t\treturn probability\n",
    "\n",
    "\tdef predict(self, test):\n",
    "\t\tpro = float(\"-inf\")\n",
    "\t\ttest = np.insert(test,0,1)\n",
    "\t\tfor label in self.labels:\n",
    "\t\t\ttem_pro = self.sigmoid(np.matmul(test,self.w_classes[label]))\n",
    "\t\t\tif tem_pro > pro:\n",
    "\t\t\t\tpro = tem_pro\n",
    "\t\t\t\tprediction = label\n",
    "\t\treturn prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parker/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:44: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"../data/digit-recognizer/data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/digit-recognizer/data/test.csv\")\n",
    "classifier = LogisticRegression(0.01,0.2,1000,1400)\n",
    "train = df_train.to_numpy()\n",
    "y = train[:,0]\n",
    "X = train[:,1:]\n",
    "classifier.fit(X,y)\n",
    "\n",
    "f = open(\"../data/digit-recognizer/outputs/submission_lr.csv\", \"w\")\n",
    "f.write(\"ImageId,Label\\n\" )\n",
    "test = df_test\n",
    "test = test.to_numpy()\n",
    "for i in range(len(test)):\n",
    "    x = test[i]\n",
    "    a = classifier.predict(x)\n",
    "    f.write(str(i+1) + \",\" + str(a) +\"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
