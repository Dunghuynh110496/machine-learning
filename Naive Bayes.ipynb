{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "## Model\n",
    "$$\\eqalign{\n",
    "  & \\bar y = \\arg \\mathop {\\max }\\limits_y p\\left( {Y = y|X = x} \\right)p = \\arg \\mathop {\\max }\\limits_y \\frac{{p\\left( {Y = y} \\right)p(X = x|Y = y)}}{{P(X = x)}}  \\cr \n",
    "  &  = \\arg \\mathop {\\max }\\limits_y p\\left( {Y = y} \\right)\\prod\\limits_{i = 1}^n {p({x_i}|Y = y) = \\arg \\mathop {\\max }\\limits_y \\ln \\left( {p\\left( {Y = y} \\right)p({x_i}|Y = y)} \\right)}   \\cr \n",
    "  &  = \\arg \\mathop {\\max }\\limits_y \\ln p\\left( {Y = y} \\right) + \\sum\\limits_{i = 1}^d {\\ln } p({x_i}|Y = y) \\cr} $$\n",
    "\n",
    "${\\bar y}$ is the prediction\n",
    "\n",
    "The first equal symbol: $y_i$ is the label that has the highest probability given vector x.\n",
    "\n",
    "The second equal symbol: it happens based on Bayes Theorem.\n",
    "\n",
    "The third equal symbol: it happens if $x_i$ is independent with each other. However, in the real life, it is impossoble. Therefore, it is called naive.\n",
    "\n",
    "\n",
    "####Class prior distribution $p(Y = y)$ is again easy to estimate from the training data:\n",
    "\n",
    "$$p\\left( {Y = y} \\right) = \\frac{{\\# Observations\\,in\\,class\\,y}}{{\\# Observations}}$$\n",
    " \n",
    "####Class-conditional distribution:\n",
    "\n",
    "\n",
    "$$\\eqalign{\n",
    "  & p\\left( {{x_i}|y} \\right) = \\frac{1}{{\\sqrt {2\\pi } {\\sigma _{y,i}}}}{e^{ - \\frac{{{{\\left( {{x_i} - {\\mu _{y,i}}} \\right)}^2}}}{{2{\\sigma _{y,i}}^2}}}}  \\cr \n",
    "  & {\\mu _{y,i}} = \\frac{{sum\\,of\\,values\\,of\\,{x_i}\\,in\\,obvervation\\,y}}{{\\# obvervation\\,of\\,y}}  \\cr \n",
    "  & {\\sigma ^2}_{y,i} = \\frac{{\\sum\\limits_{j = 1}^{\\# obvervation\\,\\,\\,of\\,\\,y} {({x_{j,i}} - {\\mu _{y,i}})} }}{{\\# obvervation\\,\\,of\\,y\\,\\,\\,\\,}} \\cr} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How to fit it\n",
    "\n",
    "\n",
    "\n",
    "## How to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from math import pi\n",
    "import numpy as np\n",
    "class NaiveBayer:\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\tdef fit(self, train):\n",
    "\t\tself.train = train\n",
    "\t\ttrain = df_train.to_numpy()\n",
    "\t\tself.summaries = {}\n",
    "\t\tself.labels = np.unique(train[:,0])\n",
    "\t\tself.labels.sort()\n",
    "\t\tfor label in self.labels:\n",
    "\t\t\ttrain_copy = train[train[:,0]==label]\n",
    "\t\t\tmatrix = train_copy[:,1:]\n",
    "\t\t\tmeans = matrix.mean(axis = 0)\n",
    "\t\t\tstd = matrix.std(axis = 0)\n",
    "\t\t\tprior = len(matrix)/len(train)\n",
    "\t\t\tself.summaries[label] = (prior, means , std)\n",
    "\t \n",
    "\tdef predict(self, x):\n",
    "\t\tpredictions = {}\n",
    "\t\tfor label in self.labels:\n",
    "\t\t\tprior_probability, mean, std = np.log(self.summaries[label][0]), self.summaries[label][1], self.summaries[label][2]\n",
    "\t\t\tfor idx in range(len(x)):\n",
    "\t\t\t\tif mean[idx] != 0:\n",
    "\t\t\t\t\tprior_probability += -((x[idx] - mean[idx])**2 / (2 * std[idx]**2 )) - np.log(std[idx])\n",
    "\t\t\tpredictions[label] = prior_probability\n",
    "\t\ta = float('-inf')\n",
    "\t\tfor key in predictions:\n",
    "\t\t\tif predictions[key] > a:\n",
    "\t\t\t\ta = predictions[key]\n",
    "\t\t\t\tprediction = key\n",
    "\t\treturn prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "classifier = NaiveBayer()\n",
    "classifier.fit(df_train)\n",
    "f = open(\"outpus/submission_NB.csv\", \"w\")\n",
    "f.write(\"ImageId,Label\\n\" )\n",
    "test = df_test\n",
    "test = test.to_numpy()\n",
    "for i in range(len(test)):\n",
    "  x = test[i]\n",
    "  a = classifier.predict(x)\n",
    "  f.write(str(i+1) + \",\" + str(a) +\"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
